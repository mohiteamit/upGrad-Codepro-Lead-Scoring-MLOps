{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1 : Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWb_Pph5GRXm"
   },
   "outputs": [],
   "source": [
    "# ignore the warnings in the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### 2 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy the cleaned data obtained from the data_cleaning notebook into the Data folder before proceeding\n",
    "%time\n",
    "data = pd.read_csv('data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the data column as it is not needed for training\n",
    "data = data.drop(['created_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3 Setting up Environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection and setup a SQLite database with the name \"lead_scoring_model_experimentation.db\" in \n",
    "# 'Assignment/02_training_pipeline/notebooks/' location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sqlite3\n",
    "\n",
    "# # Get current directory (where notebook is running)\n",
    "# db_dir = os.getcwd()  # this will be '02_training_pipeline/notebooks' if running from the notebook\n",
    "\n",
    "# # Define DB name and full path\n",
    "# db_name = 'lead_scoring_model_experimentation.db'\n",
    "# db_path = os.path.join(db_dir, db_name)\n",
    "\n",
    "# # Create a connection to the SQLite database\n",
    "# conn = sqlite3.connect(db_path)\n",
    "\n",
    "# # Print confirmation\n",
    "# print(f\"Database created at: {db_path}\")\n",
    "\n",
    "# # Close connection\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now you need to start the MLflow server in a new terminal. \n",
    "Note: Before you start the MLflow server, create a folder named mlruns in the assignment directory.\n",
    "Now you need to run the command to start MLflow server such that:\n",
    "1. The lead_scoring_model_experimentation.db which you created above is used as the backend-store.\n",
    "2. mlruns folder is used as an artifact directory. \n",
    "3. The server runs on the port 6006.\n",
    "\n",
    "The steps to do so are as follows:\n",
    "Open a new terminal.\n",
    "Then go to the Assignment directory using the cd command. Type the command: cd Assignment/\n",
    "Create a folder named mlruns here. You can create this folder using either the command line or GUI. To create this folder via the command line run the command: mkdir ./mlruns\n",
    "Then, type the following command to start the MLflow server: \n",
    "mlflow server --backend-store-uri='sqlite:///./02_training_pipeline/notebooks/lead_scoring_model_experimentation.db' --default-artifact-root=\"./mlruns\" --port=6006 --host=0.0.0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once, your server is successfully running, create a mlflow tracking uri at \"http://0.0.0.0:6006\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My working mlflow command\n",
    "\n",
    "mlflow ui   --backend-store-uri=\"sqlite:///02_training_pipeline/notebooks/lead_scoring_model_experimentation.db\"   \\\n",
    "--default-artifact-root=\"file:///home/CodePro-Lead-Scoring2/02_training_pipeline/notebooks/mlruns\"   \\\n",
    "--port=5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow ui --backend-store-uri=\"sqlite:///02_training_pipeline/notebooks/lead_scoring_model_experimentation.db\" \\\n",
    "--default-artifact-root=\"file:///home/CodePro-Lead-Scoring2/02_training_pipeline/notebooks/mlruns\" \\\n",
    "--port=5001 \\\n",
    "--host=0.0.0.0 \\\n",
    "--gunicorn-opts=\"--log-level=ERROR\" \\\n",
    "2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow ui --port=5001 --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Mlflow url\n",
    "\n",
    "https://76a7d32852700.notebooks.jarvislabs.net/proxy/5001/#/experiments/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check mlflow URI\n",
    "# import mlflow\n",
    "# print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "# print(\"Artifact URI:\", mlflow.get_artifact_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "# Setup with PyCaret 3.3.2 syntax\n",
    "exp = setup(\n",
    "    data=data,\n",
    "    target = 'app_complete_flag',  \n",
    "    fold_shuffle=True, \n",
    "    session_id = 42,\n",
    "    normalize = True, \n",
    "    transformation = True, \n",
    "    remove_multicollinearity = True, \n",
    "    multicollinearity_threshold = 0.95,\n",
    "    n_jobs=4,\n",
    "    use_gpu=False,\n",
    "    log_experiment=True,\n",
    "    # experiment_name='Lead_Scoring_Model_Experimentation',\n",
    "    log_plots=True,\n",
    "    log_data=True,\n",
    "    verbose=True,\n",
    "    log_profile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4 : Model Experimentation with pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a experimentation with pycaret and exclude ['gbc','knn','qda', 'dummy', 'svm', 'ada']\n",
    "best_model = compare_models(sort='AUC', exclude=['gbc', 'knn', 'qda', 'dummy', 'svm', 'ada'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model which gives the highest accuracy\n",
    "final_model = create_model(best_model, fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature importance plot\n",
    "plot_model(final_model, plot='feature_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(final_model, plot = 'auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(final_model, plot = 'confusion_matrix', plot_kwargs = {'percent' : True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5 : Model Experimentation after dropping features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above feature tests we can claerly see that some of the features are not significant. We will now drop all the insignificant features and select only the significant ones.\n",
    "The list of the significant features is \n",
    "['total_leads_droppped', 'city_tier', 'referred_lead', 'app_complete_flag', 'first_platform_c', 'first_utm_medium_c', 'first_utm_source_c'].\n",
    "So now you will train your model with onlly these features.\n",
    "\n",
    "Also note that in our previous experiments we saw that tree based models are our top performers. In case of tree based models we do not require transformationss normalization, scaling etc. So make sure that you use setup pycaret in the proper way. i.e, make sure that you use normalize = False and transformation = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# ['total_leads_droppped', 'city_tier', 'referred_lead', 'app_complete_flag', 'first_platform_c', 'first_utm_medium_c', 'first_utm_source_c']\n",
    "#\n",
    "# Train the model using the features listed above. Since we are using tree models we do not require any transformaions \n",
    "# such as normalization, scaling etc.So make sure that you use setup pycaret in the proper way. i.e, make sure that you use \n",
    "# normalize = False and transformation = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = ['total_leads_droppped', 'city_tier', 'referred_lead', 'app_complete_flag', \n",
    "                        'first_platform_c', 'first_utm_medium_c', 'first_utm_source_c']\n",
    "data = data[significant_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyCaret with tree-based model settings\n",
    "exp_tree = setup(\n",
    "    data=data,\n",
    "    target='app_complete_flag',\n",
    "    fold_shuffle=True,\n",
    "    session_id=42,\n",
    "    normalize=False,\n",
    "    transformation=False,\n",
    "    remove_multicollinearity=True,\n",
    "    multicollinearity_threshold=0.95,\n",
    "    n_jobs=5,\n",
    "    use_gpu=False,\n",
    "    log_experiment=True,\n",
    "    log_plots=True,\n",
    "    log_data=True,\n",
    "    verbose=True,\n",
    "    log_profile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a experimentation with pycaret and exclude ['gbc','knn','qda', 'dummy', 'svm', 'ada']. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model = compare_models(sort='AUC', exclude=['gbc', 'knn', 'qda', 'dummy', 'svm', 'ada'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should get lightgbm as the best performing model. So now we will train a lightGBM model manually using pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune the hyper parameters of the lightgbm model using optuna on 10 folds and optimise AUC as that was our system metric, \n",
    "# hence we will optimise AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'force_row_wise': [True]\n",
    "# }\n",
    "\n",
    "tuned_lightgbm = tune_model(lightgbm_model, optimize='AUC', fold=10, search_library='optuna', custom_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final models configuration so that we can use it in the model retraining pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_lightgbm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model-AfterFeature.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cd94b908b2292e9d2da2e5e75d12d6a294435c608eaff704093f5bae14b723d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "144f839f29354f668dda71aa605216ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_383709fd8fda47ceadbf243d18428cf8",
      "max": 74,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_898da34f06fa4fd3be6fc028d246a379",
      "value": 74
     }
    },
    "383709fd8fda47ceadbf243d18428cf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8239e3c9a1714d16a804cdf4239ec7f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "898da34f06fa4fd3be6fc028d246a379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94fbf501ef6e499387332ad398f2d3ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e278bc3113d41398166df961701e9fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a048df655c51475fb7369eb70702a575",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e52bcc4a1c3046bfb9e88d9e2d2218dd",
      "value": 4
     }
    },
    "a048df655c51475fb7369eb70702a575": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a18be2b61ba5486e9c132f9e5aa0e09e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94fbf501ef6e499387332ad398f2d3ff",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8239e3c9a1714d16a804cdf4239ec7f3",
      "value": 3
     }
    },
    "e52bcc4a1c3046bfb9e88d9e2d2218dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
